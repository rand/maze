"""HumanEval benchmark runner for Maze.

HumanEval: 164 hand-written Python programming problems
Dataset: https://github.com/openai/human-eval

Metrics:
- pass@1: Percentage of problems solved on first attempt
- pass@3: Percentage solved within 3 attempts
- pass@10: Percentage solved within 10 attempts

Usage:
    python benchmarks/humaneval/runner.py --sample 10
    python benchmarks/humaneval/runner.py --full
"""

import argparse
import json
import sys
import time
from pathlib import Path
from typing import Any, Dict, List

from maze.config import Config
from maze.core.pipeline import Pipeline


# Sample HumanEval problems (actual dataset would be loaded from file)
SAMPLE_PROBLEMS = [
    {
        "task_id": "HumanEval/0",
        "prompt": "from typing import List\n\n\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n    given threshold.\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n    False\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n    True\n    \"\"\"\n",
        "canonical_solution": "    for idx, elem in enumerate(numbers):\n        for idx2, elem2 in enumerate(numbers):\n            if idx != idx2:\n                distance = abs(elem - elem2)\n                if distance < threshold:\n                    return True\n\n    return False\n",
        "test": "def check(candidate):\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\n    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\n",
        "entry_point": "has_close_elements"
    },
    {
        "task_id": "HumanEval/1",
        "prompt": "from typing import List\n\n\ndef separate_paren_groups(paren_string: str) -> List[str]:\n    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\n    separate those group into separate strings and return the list of those.\n    Separate groups are balanced (each open brace is properly closed) and not nested within each other\n    Ignore any spaces in the input string.\n    >>> separate_paren_groups('( ) (( )) (( )( ))')\n    ['()', '(())', '(()())']\n    \"\"\"\n",
        "canonical_solution": "    result = []\n    current_string = []\n    current_depth = 0\n\n    for c in paren_string:\n        if c == '(':\n            current_depth += 1\n            current_string.append(c)\n        elif c == ')':\n            current_depth -= 1\n            current_string.append(c)\n\n            if current_depth == 0:\n                result.append(''.join(current_string))\n                current_string.clear()\n\n    return result\n",
        "test": "def check(candidate):\n    assert candidate('(()()) ((())) () ((())()())') == ['(()())', '((()))', '()', '((())()())']\n    assert candidate('() (()) ((())) (((())))') == ['()', '(())', '((()))', '(((())))']\n",
        "entry_point": "separate_paren_groups"
    },
]


class HumanEvalRunner:
    """Runner for HumanEval benchmark."""

    def __init__(self, config: Optional[Config] = None):
        """Initialize benchmark runner.

        Args:
            config: Maze configuration
        """
        self.config = config or Config()
        self.config.project.language = "python"
        self.pipeline = Pipeline(self.config)

    def run_problem(self, problem: Dict[str, Any], attempts: int = 1) -> Dict[str, Any]:
        """Run single HumanEval problem.

        Args:
            problem: Problem dictionary
            attempts: Number of generation attempts

        Returns:
            Result dictionary with pass/fail and metadata
        """
        task_id = problem["task_id"]
        prompt = problem["prompt"]
        test_code = problem["test"]
        entry_point = problem["entry_point"]

        print(f"Running {task_id}...")

        results = []
        for attempt in range(attempts):
            start = time.time()

            # Generate solution
            gen_result = self.pipeline.run(prompt)
            
            duration = time.time() - start

            # Validate syntax
            syntax_valid = gen_result.validation.syntax_valid if gen_result.validation else False

            # For now, record result without execution
            # (Full implementation would execute test in sandbox)
            results.append({
                "attempt": attempt + 1,
                "generated_code": gen_result.code,
                "syntax_valid": syntax_valid,
                "duration_seconds": duration,
                "success": gen_result.success,
            })

        return {
            "task_id": task_id,
            "attempts": results,
            "best_attempt": max(results, key=lambda x: x["syntax_valid"]),
        }

    def run_benchmark(self, problems: List[Dict[str, Any]], k_values: List[int] = [1, 3, 10]) -> Dict[str, Any]:
        """Run benchmark on multiple problems.

        Args:
            problems: List of problem dictionaries
            k_values: List of k values for pass@k metric

        Returns:
            Benchmark results
        """
        print(f"Running HumanEval benchmark on {len(problems)} problems...")
        print("=" * 60)

        results = []
        for problem in problems:
            max_k = max(k_values)
            result = self.run_problem(problem, attempts=max_k)
            results.append(result)

        # Compute metrics
        metrics = {
            "total_problems": len(problems),
            "results": results,
        }

        # Compute pass@k (simplified - just syntax valid for now)
        for k in k_values:
            syntax_valid_count = sum(
                1 for r in results
                if any(attempt["syntax_valid"] for attempt in r["attempts"][:k])
            )
            metrics[f"pass@{k}"] = syntax_valid_count / len(problems) if problems else 0

        return metrics


def main() -> int:
    """Main benchmark entry point.

    Returns:
        Exit code
    """
    parser = argparse.ArgumentParser(description="Run HumanEval benchmark")
    parser.add_argument(
        "--sample",
        type=int,
        default=2,
        help="Number of sample problems to run (default: 2, use --full for all)",
    )
    parser.add_argument(
        "--full",
        action="store_true",
        help="Run full benchmark (164 problems)",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=Path("benchmarks/humaneval/results.json"),
        help="Output file for results",
    )

    args = parser.parse_args()

    # Select problems
    if args.full:
        # Would load full dataset
        print("Full HumanEval not yet implemented (requires dataset)")
        print("Running sample problems...")
        problems = SAMPLE_PROBLEMS
    else:
        problems = SAMPLE_PROBLEMS[:args.sample]

    # Run benchmark
    runner = HumanEvalRunner()
    results = runner.run_benchmark(problems, k_values=[1, 3])

    # Print results
    print("\n" + "=" * 60)
    print("HumanEval Results")
    print("=" * 60)
    print(f"Total problems: {results['total_problems']}")
    print(f"pass@1: {results['pass@1']:.1%}")
    print(f"pass@3: {results['pass@3']:.1%}")

    # Save results
    args.output.parent.mkdir(parents=True, exist_ok=True)
    with open(args.output, "w") as f:
        json.dump(results, f, indent=2)

    print(f"\nâœ… Results saved to {args.output}")

    return 0


if __name__ == "__main__":
    sys.exit(main())
